{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beec3fdf-884c-4ba2-9187-1768bef216a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model import NetV2, MCMC\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "from NISP import NISP\n",
    "import torch.nn.utils.prune as prune\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cf5a6",
   "metadata": {},
   "source": [
    "### Fixed-Masks with bayesian inference for training\n",
    "This notebook evaluates the performance of the performance of using fixed dropout masks and evaluating a weighted average of them at inference time.\n",
    "\n",
    "This notebook shows training and pruning for one of the better set of hyperparameters that also had higher dropout probabilities on each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736030d",
   "metadata": {},
   "source": [
    "#### We apply random and rotations and blur to make task harder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd47a49-430c-4746-8a71-37953fa95bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=50),\n",
    "    transforms.ToTensor(), transforms.GaussianBlur(kernel_size=5, sigma=(4, 5)),  # Stronger blur\n",
    "    transforms.Lambda(lambda x: torch.flatten(x)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3c082b-1ba0-43dc-bfe5-79b01d2b9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c569fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "NUM_MASKS = 10\n",
    "LR = 0.001\n",
    "dropout_probs=[0.7, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decc1b4c-7e4d-4e68-a60c-ac114500ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "indices = torch.randperm(len(dataset1)).tolist()  # Shuffled once\n",
    "\n",
    "# Use SubsetRandomSampler to keep the same shuffle order on each epoch for the over-fitting step\n",
    "sampler = SequentialSampler(indices)  # Keeps the order fixed\n",
    "train_dataloader = DataLoader(dataset1, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "test_dataloader = DataLoader(dataset2, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f335f9",
   "metadata": {},
   "source": [
    "### Overfitting portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b234997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iteration = 0\n",
    "if not os.path.exists(f\"../model{model_iteration}.pth\"):\n",
    "    model = NetV2(num_masks=NUM_MASKS, dropout_probs=dropout_probs)\n",
    "    opt = Adam(model.parameters(), lr=LR)\n",
    "    lossFn = torch.nn.NLLLoss() # Use NLL since we our model is outputting a probability\n",
    "    torch.save(model.state_dict(), f\"../model{model_iteration}.pth\")\n",
    "model_iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a859db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"../model{model_iteration}.pth\"):\n",
    "    NGROUPS = 10 # dividing groups to use\n",
    "    mask_groups = [list(range(i, NUM_MASKS, NGROUPS)) for i in range(NGROUPS)]  # Partition masks\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        trainCorrect = 0\n",
    "        totalLoss = 0\n",
    "        tot = 0\n",
    "        for idx, (x, y) in tqdm(enumerate(train_dataloader)):\n",
    "            group_id = idx % NGROUPS  # Assign batch to a group\n",
    "            masks = mask_groups[group_id]  # Get all masks in this group\n",
    "            \n",
    "            for mask in masks:\n",
    "                logits = model.forward(x, mask=mask)\n",
    "                loss = lossFn(logits, y)\n",
    "                totalLoss += loss.item()\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                trainCorrect += (logits.argmax(1) == y).type(torch.float).sum().item()\n",
    "                tot += len(y)\n",
    "\n",
    "        print(f\"Train Accuracy: {trainCorrect} / {tot}: {trainCorrect / tot}\")\n",
    "        print(f\"Total loss: {totalLoss}\")\n",
    "    torch.save(model.state_dict(), f\"../model{model_iteration}.pth\")\n",
    "    model1 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c76d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc8ebf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetV2(\n",
       "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (dropout1): ConsistentMCDropout(p=0.7)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (dropout2): ConsistentMCDropout(p=0.7)\n",
       "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NetV2(num_masks=NUM_MASKS, dropout_probs=dropout_probs)\n",
    "model.load_state_dict(torch.load(f\"../model{1}.pth\"), strict=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e19ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = MCMC(model=model, increment_amt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d085d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "train_dataloader = DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3892a64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/kunalkapur/Workspace/bayesian-ensemble/experiments-deep/../model.py:257: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Scalar.cpp:23.)\n",
      "  acceptance_prob = torch.tensor([min(torch.tensor([1]).float(), ratio)])\n",
      "1875it [00:12, 147.43it/s]\n",
      "1875it [00:13, 140.21it/s]\n",
      "1875it [00:12, 148.63it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i in range(3):\n",
    "    trainCorrect = 0\n",
    "    for idx, (x, y)  in tqdm(enumerate(train_dataloader)):\n",
    "        mcmc.transition(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a86dd5",
   "metadata": {},
   "source": [
    "### Distribution ends up being close to uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a9974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000])\n"
     ]
    }
   ],
   "source": [
    "dist = torch.tensor([(val / mcmc.tot).item() for val in mcmc.ocurrences])\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6a66ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m values, indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(\u001b[43mdist\u001b[49m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(indices)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(indices\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dist' is not defined"
     ]
    }
   ],
   "source": [
    "values, indices = torch.topk(dist, k=10)\n",
    "print(indices)\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65dc967b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/kunalkapur/Workspace/bayesian-ensemble/experiments-deep/../model.py:284: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  keep_indices = torch.tensor(keep_indices, dtype=torch.long)\n",
      "10000it [00:28, 350.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with all masks: 0.9436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_correct_top_3 = 0\n",
    "test_correct = 0\n",
    "model.eval()\n",
    "for idx, (x, y)  in tqdm(enumerate(test_dataloader)):\n",
    "    # logits2 = model.forward(x, mask=1)\n",
    "    logits = mcmc.predict(x, chosen_masks=indices)\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    test_correct += (pred == y).sum().item()\n",
    "\n",
    "\n",
    "    logits = mcmc.predict(x, chosen_masks=indices)\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    test_correct_top_3 += (pred == y).sum().item()\n",
    "print(f\"Test accuracy with all masks: {test_correct / len(dataset2)}\")\n",
    "# print(f\"Test accuracy with top 3: {test_correct_top_3 / len(dataset2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76628139",
   "metadata": {},
   "source": [
    "# Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54216dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "from torch import nn\n",
    "\n",
    "def apply_nisp_pruning(model: nn.Module, layer_name: str, neuron_mask: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Applies structured pruning to a given layer using a neuron-wise mask.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model.\n",
    "        layer_name: Name of the layer to prune (e.g., 'fc1').\n",
    "        neuron_mask: 1D tensor with 1s for neurons to keep, 0s to prune (length = out_features).\n",
    "    \"\"\"\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    \n",
    "    if not isinstance(layer, nn.Linear):\n",
    "        raise ValueError(\"Only Linear layers are supported for this kind of pruning.\")\n",
    "\n",
    "    weight_mask = neuron_mask[:, None].expand_as(layer.weight.data)\n",
    "\n",
    "    # Use custom mask with PyTorch's pruning\n",
    "    prune.CustomFromMask.apply(layer, name='weight', mask=weight_mask)\n",
    "\n",
    "    # Optionally prune the bias too\n",
    "    if layer.bias is not None:\n",
    "        bias_mask = neuron_mask.clone()\n",
    "        prune.CustomFromMask.apply(layer, name='bias', mask=bias_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ae8a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layers = {\n",
    "    'fc1': model.dropout1,\n",
    "    'fc2': model.dropout2,\n",
    "}\n",
    "\n",
    "nisp = NISP(model)\n",
    "importance_scores = nisp.compute_aggregated_importance_scores(dropout_layers)\n",
    "# Let's say you've already computed the scores and pruning mask:\n",
    "fc1_scores = importance_scores['fc1']\n",
    "fc1_mask = nisp.get_pruning_mask(fc1_scores, pruning_rate=0.5)\n",
    "apply_nisp_pruning(model, 'fc1', fc1_mask)\n",
    "\n",
    "\n",
    "fc2_scores = importance_scores['fc2']\n",
    "fc2_mask = nisp.get_pruning_mask(fc2_scores, pruning_rate=0.4)\n",
    "apply_nisp_pruning(model, 'fc2', fc2_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0712ab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(513)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.bias.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b828202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=512, bias=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.remove(model.fc1, 'weight')\n",
    "prune.remove(model.fc1, 'bias')\n",
    "prune.remove(model.fc2, 'weight')\n",
    "prune.remove(model.fc2, 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c6a0c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(242256)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb38cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = NetV2(num_masks=5, dropout_probs=[0.4, 0.4])\n",
    "initial_model.load_state_dict(torch.load(f\"../model{0}.pth\"))\n",
    "initial_model.num_masks = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "29037c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_mask_weight = final_mask1.bool()[:, None].expand_as(initial_model.fc1.weight)\n",
    "prune_mask_bias = final_mask1.bool() if initial_model.fc1.bias is not None else None\n",
    "\n",
    "prune.custom_from_mask(\n",
    "    module=initial_model.fc1,\n",
    "    name='weight',\n",
    "    mask=prune_mask_weight\n",
    ")\n",
    "\n",
    "if prune_mask_bias is not None:\n",
    "    prune.custom_from_mask(\n",
    "        module=initial_model.fc1,\n",
    "        name='bias',\n",
    "        mask=prune_mask_bias\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94bb3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_mask_weight = final_mask2.bool()[:, None].expand_as(initial_model.fc2.weight)\n",
    "prune_mask_bias = final_mask2.bool() if initial_model.fc2.bias is not None else None\n",
    "\n",
    "prune.custom_from_mask(\n",
    "    module=initial_model.fc2,\n",
    "    name='weight',\n",
    "    mask=prune_mask_weight\n",
    ")\n",
    "\n",
    "if prune_mask_bias is not None:\n",
    "    prune.custom_from_mask(\n",
    "        module=initial_model.fc2,\n",
    "        name='bias',\n",
    "        mask=prune_mask_bias\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab164d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=512, bias=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.remove(initial_model.fc1, 'weight')\n",
    "prune.remove(initial_model.fc2, 'weight')\n",
    "prune.remove(initial_model.fc1, 'bias')\n",
    "prune.remove(initial_model.fc2, 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4974d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(411)\n",
      "tensor(206)\n"
     ]
    }
   ],
   "source": [
    "print(initial_model.fc1.bias.count_nonzero())\n",
    "print(initial_model.fc2.bias.count_nonzero())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30801eaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initial_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43minitial_model\u001b[49m\n\u001b[1;32m      2\u001b[0m opt \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR)\n\u001b[1;32m      3\u001b[0m lossFn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mNLLLoss() \u001b[38;5;66;03m# Use NLL since we our model is outputting a probability\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initial_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = initial_model\n",
    "opt = Adam(model.parameters(), lr=LR)\n",
    "lossFn = torch.nn.NLLLoss() # Use NLL since we our model is outputting a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d73635",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initial_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model  \u001b[38;5;241m=\u001b[39m \u001b[43minitial_model\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_iteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m     NGROUPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;66;03m# dividing groups to use\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initial_model' is not defined"
     ]
    }
   ],
   "source": [
    "model  = initial_model\n",
    "if not os.path.exists(f\"../model{model_iteration}.pth\"):\n",
    "    NGROUPS = 5 # dividing groups to use\n",
    "    mask_groups = [list(range(i, NUM_MASKS, NGROUPS)) for i in range(NGROUPS)]  # Partition masks\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        trainCorrect = 0\n",
    "        totalLoss = 0\n",
    "        tot = 0\n",
    "        for idx, (x, y) in tqdm(enumerate(train_dataloader)):\n",
    "            group_id = idx % NGROUPS  # Assign batch to a group\n",
    "            masks = mask_groups[group_id]  # Get all masks in this group\n",
    "            \n",
    "            for mask in masks:\n",
    "                logits = model.forward(x, mask=mask)\n",
    "                loss = lossFn(logits, y)\n",
    "                totalLoss += loss.item()\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                trainCorrect += (logits.argmax(1) == y).type(torch.float).sum().item()\n",
    "                tot += len(y)\n",
    "\n",
    "        print(f\"Train Accuracy: {trainCorrect} / {tot}: {trainCorrect / tot}\")\n",
    "        print(f\"Total loss: {totalLoss}\")\n",
    "    torch.save(model.state_dict(), f\"../model{model_iteration}.pth\")\n",
    "    model1 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iteration = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98b9a326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(411)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NetV2(num_masks=5, dropout_probs=dropout_probs)\n",
    "model.load_state_dict(torch.load(f\"../model{2}.pth\"), strict=True)\n",
    "model.eval()\n",
    "model.fc1.bias.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664ce297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc = MCMC(model=model, increment_amt=10)\n",
    "train_dataloader = DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model.num_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef0753a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [00:09, 195.21it/s]\n",
      "1875it [00:09, 205.66it/s]\n",
      "1875it [00:09, 205.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2983, 0.1273, 0.1347, 0.1483, 0.2914])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i in range(3):\n",
    "    trainCorrect = 0\n",
    "    for idx, (x, y) in tqdm(enumerate(train_dataloader)):\n",
    "        mcmc.transition(x=x, y=y)\n",
    "dist = torch.tensor([(val / mcmc.tot).item() for val in mcmc.ocurrences])\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac656b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(mcmc.num_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c84231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 3, 2, 1])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.topk(dist, k=5)\n",
    "print(indices)\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26cf67ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/kunalkapur/Workspace/bayesian-ensemble/experiments-deep/../model.py:284: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  keep_indices = torch.tensor(keep_indices, dtype=torch.long)\n",
      "10000it [00:16, 613.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with all masks: 0.9552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dist = torch.tensor([(val / mcmc.tot).item() for val in mcmc.ocurrences])\n",
    "test_correct_top_3 = 0\n",
    "test_correct = 0\n",
    "model.eval()\n",
    "for idx, (x, y)  in tqdm(enumerate(test_dataloader)):\n",
    "    # logits2 = model.forward(x, mask=1)\n",
    "    logits = mcmc.predict(x, chosen_masks=indices)\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    test_correct += (pred == y).sum().item()\n",
    "\n",
    "\n",
    "    logits = mcmc.predict(x, chosen_masks=indices)\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    test_correct_top_3 += (pred == y).sum().item()\n",
    "print(f\"Test accuracy with all masks: {test_correct / len(dataset2)}\")\n",
    "# print(f\"Test accuracy with top 3: {test_correct_top_3 / len(dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f5713d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nisp = NISP(model)\n",
    "importance_scores = nisp.compute_importance_scores()\n",
    "prune_mask1 = nisp.get_pruning_mask('fc1', 0.4)\n",
    "prune_mask2 = nisp.get_pruning_mask('fc2', 0.3)\n",
    "\n",
    "fc1_majority_mask = model.dropout1.get_majority_vote_mask(threshold=0.5)\n",
    "fc2_majority_mask = model.dropout2.get_majority_vote_mask(threshold=0.5)\n",
    "\n",
    "\n",
    "def conservative_combine(nisp_mask, dropout_mask):\n",
    "    \"\"\"MOST AGGRESSIVE PRUNING: Only keep if both methods agree\"\"\"\n",
    "    return (nisp_mask.bool() & dropout_mask.bool()).float()  # 1=keep, 0=prune\n",
    "\n",
    "final_mask1 = conservative_combine(prune_mask1, fc1_majority_mask)  # Uses &\n",
    "final_mask2 = conservative_combine(prune_mask2, fc2_majority_mask)  # Uses &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53d552e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(311)\n",
      "tensor(614)\n",
      "tensor(188)\n",
      "tensor(112)\n"
     ]
    }
   ],
   "source": [
    "print(fc1_majority_mask.count_nonzero())\n",
    "print(prune_mask1.count_nonzero())\n",
    "print(final_mask1.count_nonzero())\n",
    "print(final_mask2.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bac2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = NetV2(num_masks=1, dropout_probs=[0.0, 0.0])\n",
    "initial_model.load_state_dict(torch.load(f\"../model{0}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77e316ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_mask_weight = final_mask1.bool()[:, None].expand_as(initial_model.fc1.weight)\n",
    "prune_mask_bias = final_mask1.bool() if initial_model.fc1.bias is not None else None\n",
    "\n",
    "prune.custom_from_mask(\n",
    "    module=initial_model.fc1,\n",
    "    name='weight',\n",
    "    mask=prune_mask_weight\n",
    ")\n",
    "\n",
    "if prune_mask_bias is not None:\n",
    "    prune.custom_from_mask(\n",
    "        module=initial_model.fc1,\n",
    "        name='bias',\n",
    "        mask=prune_mask_bias\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe49a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_mask_weight = final_mask2.bool()[:, None].expand_as(initial_model.fc2.weight)\n",
    "prune_mask_bias = final_mask2.bool() if initial_model.fc2.bias is not None else None\n",
    "\n",
    "prune.custom_from_mask(\n",
    "    module=initial_model.fc2,\n",
    "    name='weight',\n",
    "    mask=prune_mask_weight\n",
    ")\n",
    "\n",
    "if prune_mask_bias is not None:\n",
    "    prune.custom_from_mask(\n",
    "        module=initial_model.fc2,\n",
    "        name='bias',\n",
    "        mask=prune_mask_bias\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f5c414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(188)\n",
      "tensor(112)\n"
     ]
    }
   ],
   "source": [
    "prune.remove(initial_model.fc1, 'weight')\n",
    "prune.remove(initial_model.fc2, 'weight')\n",
    "prune.remove(initial_model.fc1, 'bias')\n",
    "prune.remove(initial_model.fc2, 'bias')\n",
    "\n",
    "print(initial_model.fc1.bias.count_nonzero())\n",
    "print(initial_model.fc2.bias.count_nonzero())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d673ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iteration = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d3034f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initial_model\n",
    "opt = Adam(model.parameters(), lr=LR)\n",
    "lossFn = torch.nn.NLLLoss() # Use NLL since we our model is outputting a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31050a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:13, 25.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 547479.0 / 600000: 0.912465\n",
      "Total loss: 5760.077270619338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:14, 25.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 563824.0 / 600000: 0.9397066666666667\n",
      "Total loss: 4242.937383973622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 564214.0 / 600000: 0.9403566666666666\n",
      "Total loss: 4224.361388082732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:22, 22.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 565901.0 / 600000: 0.9431683333333334\n",
      "Total loss: 3988.9579778346233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:22, 22.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 565209.0 / 600000: 0.942015\n",
      "Total loss: 4038.8632180804852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:23, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 565722.0 / 600000: 0.94287\n",
      "Total loss: 4001.72815497173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:21, 22.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 567245.0 / 600000: 0.9454083333333333\n",
      "Total loss: 3884.2582408610033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 23.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 567426.0 / 600000: 0.94571\n",
      "Total loss: 3827.468564099603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:22, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 567423.0 / 600000: 0.945705\n",
      "Total loss: 3847.038970327354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:37, 19.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 567941.0 / 600000: 0.9465683333333333\n",
      "Total loss: 3808.8674488131655\n"
     ]
    }
   ],
   "source": [
    "model  = initial_model\n",
    "if not os.path.exists(f\"../model{model_iteration}.pth\"):\n",
    "    NGROUPS = 1 # dividing groups to use\n",
    "    mask_groups = [list(range(i, NUM_MASKS, NGROUPS)) for i in range(NGROUPS)]  # Partition masks\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        trainCorrect = 0\n",
    "        totalLoss = 0\n",
    "        tot = 0\n",
    "        for idx, (x, y) in tqdm(enumerate(train_dataloader)):\n",
    "            group_id = idx % NGROUPS  # Assign batch to a group\n",
    "            masks = mask_groups[group_id]  # Get all masks in this group\n",
    "            \n",
    "            for mask in masks:\n",
    "                logits = model.forward(x, mask=mask)\n",
    "                loss = lossFn(logits, y)\n",
    "                totalLoss += loss.item()\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                trainCorrect += (logits.argmax(1) == y).type(torch.float).sum().item()\n",
    "                tot += len(y)\n",
    "\n",
    "        print(f\"Train Accuracy: {trainCorrect} / {tot}: {trainCorrect / tot}\")\n",
    "        print(f\"Total loss: {totalLoss}\")\n",
    "    torch.save(model.state_dict(), f\"../model{model_iteration}.pth\")\n",
    "    model1 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c10ecb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(188)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NetV2(num_masks=1, dropout_probs=[0.0, 0.0])\n",
    "model.load_state_dict(torch.load(f\"../model{3}.pth\"), strict=True)\n",
    "model.eval()\n",
    "model.fc1.bias.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "366ec4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc = MCMC(model=model, increment_amt=10)\n",
    "train_dataloader =DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model.num_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af4c1976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [00:08, 211.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i in range(1):\n",
    "    trainCorrect = 0\n",
    "    for idx, (x, y) in tqdm(enumerate(train_dataloader)):\n",
    "        mcmc.transition(x=x, y=y)\n",
    "dist = torch.tensor([(val / mcmc.tot).item() for val in mcmc.ocurrences])\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e98517a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.topk(dist, k=1)\n",
    "print(indices)\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9ea5f1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/kunalkapur/Workspace/bayesian-ensemble/experiments-deep/../model.py:284: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  keep_indices = torch.tensor(keep_indices, dtype=torch.long)\n",
      "10000it [00:04, 2135.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with all masks: 0.9386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_correct_top_3 = 0\n",
    "test_correct = 0\n",
    "model.eval()\n",
    "for idx, (x, y)  in tqdm(enumerate(test_dataloader)):\n",
    "    # logits2 = model.forward(x, mask=1)\n",
    "    logits = mcmc.predict(x, chosen_masks=indices)\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    test_correct += (pred == y).sum().item()\n",
    "\n",
    "\n",
    "    logits = mcmc.predict(x, chosen_masks=indices)\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    test_correct_top_3 += (pred == y).sum().item()\n",
    "print(f\"Test accuracy with all masks: {test_correct / len(dataset2)}\")\n",
    "# print(f\"Test accuracy with top 3: {test_correct_top_3 / len(dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a31b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 784])\n",
      "tensor(147392)\n",
      "torch.Size([1024])\n",
      "tensor(188)\n",
      "tensor(112)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight.shape)\n",
    "print(model.fc1.weight.count_nonzero())\n",
    "print(model.fc1.bias.shape)\n",
    "print(model.fc1.bias.count_nonzero())\n",
    "print(model.fc2.bias.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186a774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
